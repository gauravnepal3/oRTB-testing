version: "3.9"

services:
  prometheus:
    image: prom/prometheus
    ports: [ "9090:9090" ]
    extra_hosts:
      - "host.docker.internal:host-gateway"   # Linux host-gateway mapping
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
  kafka:
    image: apache/kafka:4.1.0
    container_name: kafka
    ports:
      - "9094:9094"
    environment:
      KAFKA_NODE_ID: "1"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENERS: "PLAINTEXT://:29092,EXTERNAL://:9094,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:29092,EXTERNAL://localhost:9094"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_CLUSTER_ID: "q1shixmKQ9O5aH3hRkJv3g"
      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
      KAFKA_NUM_PARTITIONS: "3"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: "0"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'bin/kafka-broker-api-versions.sh --bootstrap-server localhost:29092 >/dev/null 2>&1'"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 90s
    volumes: [ "kafka_data:/var/lib/kafka/data" ]
    restart: unless-stopped

  clickhouse:
    image: clickhouse/clickhouse-server:head-alpine
    container_name: clickhouse
    ports: [ "8123:8123", "9000:9000" ]
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: topSecretPassword
      CLICKHOUSE_DB: default
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --user=default --password=topSecretPassword --query='SELECT 1' >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 60s
    volumes: [ "clickhouse_data:/var/lib/clickhouse" ]
    restart: unless-stopped

  rtb-app:
    build:
      context: .
      dockerfile: Dockerfile
    image: ortb-testing-app:latest
    container_name: rtb-app
    ports:
      - "8080:8080"
    network_mode: "host"
    # DO NOT use host network on Mac
    environment:
      KAFKA_BOOTSTRAP: localhost:9094
      KAFKA_TOPIC_REQUESTS: rtb_requests
      KAFKA_TOPIC_RESPONSES: rtb_responses
      KAFKA_ACKS: "0"
      KAFKA_COMPRESSION: lz4
      KAFKA_BATCHBYTES: "262144"
      KAFKA_LINGERMS: "5"

      # ClickHouse via service name + async insert + short timeouts
      CLICKHOUSE_URL: jdbc:ch://localhost:8123/default?async_insert=1&wait_for_async_insert=0&compress=1&decompress=1&socket_timeout=5000&connection_timeout=3000
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: topSecretPassword

      # Bidders on your HOST (not compose)
      EXTERNAL_TARGETS: "http://localhost:9100/bid,http://localhost:9101/bid,http://localhost:9102/bid,http://localhost:9103/bid,http://localhost:9104/bid,http://localhost:9100/bid,http://localhost:9101/bid,http://localhost:9102/bid,http://localhost:9103/bid,http://localhost:9104/bid"

      # Fanout size and a hard cap of concurrent *in-flight* fanouts
      FANOUT_COUNT: "10"                # if you want 10; reuse ports by repeating in EXTERNAL_TARGETS if needed
      FANOUT_MAX_INFLIGHT: "1800"       # <â€” backpressure (see OpenRtbController)
      AUCTION_IMMEDIATEACK: "true"
      SPRING_THREADS_VIRTUAL_ENABLED: "true"

      # Netty / client tuning (small aggregator & capture)
      JAVA_TOOL_OPTIONS: >-
        -Djava.net.preferIPv4Stack=true
        -Dnetty.pool.maxPerHost=4000
        -Dnetty.pool.maxPending=50000
        -Dnetty.pool.acquireTimeoutMs=150
        -Dnetty.client.aggMaxBytes=262144
        -Dnetty.connect.ms=100
        -Dresp.max.bytes=65536
        -XX:+UseG1GC -XX:MaxGCPauseMillis=50
    depends_on:
      kafka:
        condition: service_started
      clickhouse:
        condition: service_healthy
    restart: unless-stopped
volumes:
  kafka_data:
  clickhouse_data: